{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "917606f0-7234-4266-91d5-6b599e84adf0",
   "metadata": {},
   "source": [
    "## 19th March Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5954ca-4e2f-4676-8fa8-0cd9683afab0",
   "metadata": {},
   "source": [
    "## Q:1:- What is Min-Max scailing, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee84705-a22f-42b2-bdae-f9462df4fc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans:- \n",
    "MinMax scaling, also known as min-max normalization or feature scaling, is\n",
    "a data preprocessing technique used to transform numeric data into a fixed \n",
    "range. It rescales the data to a specific interval, typically between 0 and\n",
    "1. The purpose of Min-Max scaling is to bring all the features on the same \n",
    "scale, which can be beneficial for certain algorithms that rely on the \n",
    "magnitude of the features.\n",
    "\n",
    "The formula for Min-Max scaling is as follows:\n",
    "\n",
    "scaled_value = (value - min_value) / (max_value - min_value)\n",
    "\n",
    "where:\n",
    "\n",
    "value is the original value of a feature\n",
    "min_value is the minimum value of that feature in the dataset\n",
    "max_value is the maximum value of that feature in the dataset\n",
    "scaled_value is the rescaled value of the feature within the range [0, 1]\n",
    "The process involves calculating the minimum and maximum values of each feature\n",
    "in the dataset and then applying the above formula to scale each value accordingly.\n",
    "This ensures that the minimum value becomes 0, the maximum value becomes 1, and\n",
    "all other values are proportionally adjusted within that range.\n",
    "\n",
    "Here's an example to illustrate the application of Min-Max scaling:\n",
    "\n",
    "Suppose we have a dataset consisting of the heights (in centimeters) and weights\n",
    "(in kilograms) of individuals:\n",
    "\n",
    "Height (cm)\tWeight (kg)\n",
    "160\t55\n",
    "175\t70\n",
    "150\t45\n",
    "180\t80\n",
    "165\t60\n",
    "To apply Min-Max scaling to this dataset, we first find the minimum and maximum \n",
    "values for each feature:\n",
    "\n",
    "Minimum height: 150 cm\n",
    "Maximum height: 180 cm\n",
    "\n",
    "Minimum weight: 45 kg\n",
    "Maximum weight: 80 kg\n",
    "\n",
    "Next, we use the formula mentioned earlier to scale the values:\n",
    "\n",
    "Scaled height = (Height - Minimum height) / (Maximum height - Minimum height)\n",
    "Scaled weight = (Weight - Minimum weight) / (Maximum weight - Minimum weight)\n",
    "\n",
    "Applying this formula to the dataset, we get the following scaled values:\n",
    "\n",
    "Scaled Height\tScaled Weight\n",
    "0.2857\t0.2857\n",
    "0.8571\t0.7143\n",
    "0.0000\t0.0000\n",
    "1.0000\t1.0000\n",
    "0.4286\t0.4286\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315f7137-17eb-4e5e-804b-52b95fe87e69",
   "metadata": {},
   "source": [
    "## Q:2:- What is the Unit Vector technique in feature scailing , and how does it iffer from Min-Max scailing? Provide an example to illustrate its application ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65616f2c-033a-43de-841d-54ef1db5ecbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans:-\n",
    "In feature scaling, the unit vector technique, also known as normalization\n",
    "or vector normalization, is a method used to scale features in a dataset.\n",
    "It involves transforming the data so that each feature has a magnitude of 1.\n",
    "This technique is particularly useful when the magnitude of different features\n",
    "varies widely in the dataset.\n",
    "\n",
    "The unit vector technique scales each feature vector by dividing it by its\n",
    "magnitude (also known as the Euclidean norm or L2 norm). The magnitude of a\n",
    "feature vector is calculated as the square root of the sum of the squares of\n",
    "its individual values. After scaling, the feature vector will have a magnitude\n",
    "of 1, and the direction of the vector will be preserved.\n",
    "\n",
    "On the other hand, Min-Max scaling (also known as normalization or rescaling)\n",
    "is a different technique used to scale features. It transforms the data to a\n",
    "fixed range, usually between 0 and 1. Min-Max scaling subtracts the minimum \n",
    "value of the feature and divides it by the range (maximum value minus minimum value).\n",
    "This technique ensures that all features have the same scale and preserves the\n",
    "relative relationships between different data points.\n",
    "\n",
    "To illustrate their application, let's consider an example. Suppose we have a\n",
    "dataset of housing prices with two features: square footage \n",
    "(ranging from 500 to 3000) and number of bedrooms (ranging from 1 to 5). We \n",
    "want to scale these features using both the unit vector technique and Min-Max scaling.\n",
    "\n",
    "Unit Vector Technique:\n",
    "\n",
    "Original square footage: [1000, 2000, 1500]\n",
    "Original number of bedrooms: [2, 3, 4]\n",
    "First, we calculate the magnitude of each feature vector:\n",
    "\n",
    "Magnitude of square footage: sqrt(1000^2 + 2000^2 + 1500^2) ≈ 2795.92\n",
    "Magnitude of number of bedrooms: sqrt(2^2 + 3^2 + 4^2) ≈ 5.39\n",
    "Then, we divide each feature vector by its magnitude to obtain the scaled vectors:\n",
    "\n",
    "Scaled square footage: [1000/2795.92, 2000/2795.92, 1500/2795.92] ≈ [0.36, 0.72, 0.54]\n",
    "Scaled number of bedrooms: [2/5.39, 3/5.39, 4/5.39] ≈ [0.37, 0.55, 0.74]\n",
    "Min-Max Scaling:\n",
    "\n",
    "Original square footage: [1000, 2000, 1500]\n",
    "Original number of bedrooms: [2, 3, 4]\n",
    "For square footage:\n",
    "\n",
    "Minimum value: 1000\n",
    "Maximum value: 2000\n",
    "For number of bedrooms:\n",
    "\n",
    "Minimum value: 2\n",
    "Maximum value: 4\n",
    "Applying the Min-Max scaling formula:\n",
    "\n",
    "Scaled square footage: [(1000-1000)/(2000-1000), (2000-1000)/(2000-1000),\n",
    "                        (1500-1000)/(2000-1000)] = [0, 1, 0.5]\n",
    "Scaled number of bedrooms: [(2-2)/(4-2), (3-2)/(4-2), (4-2)/(4-2)] = [0, 0.5, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef36739-389d-47bf-a596-e8d60b8cebb9",
   "metadata": {},
   "source": [
    "## Q:3:- What is PCA (Principal Component Analysis) , and how is it used in dimensionally reduction? Provide on example to illustrate its application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8f88f2-d429-43ac-bc34-cd41618c09cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans:-\n",
    "Principal Component Analysis (PCA) is a statistical technique used for\n",
    "dimensionality reduction in data analysis and machine learning. It aims\n",
    "to transform a high-dimensional dataset into a lower-dimensional space\n",
    "while preserving the most important information or patterns present in the data.\n",
    "\n",
    "In PCA, the original features of a dataset are transformed into a new set\n",
    "of uncorrelated variables called principal components. These principal \n",
    "components are linear combinations of the original features and are ordered\n",
    "in terms of the amount of variance they explain in the data. The first\n",
    "principal component explains the largest amount of variance, the second \n",
    "explains the second largest amount, and so on.\n",
    "\n",
    "PCA is commonly used for dimensionality reduction because it helps to \n",
    "reduce the number of features or variables in a dataset while retaining\n",
    "most of the important information. By eliminating redundant or less\n",
    "informative variables, PCA can simplify the analysis, improve computational\n",
    "efficiency, and remove multicollinearity issues.\n",
    "\n",
    "Here's an example to illustrate the application of PCA in dimensionality\n",
    "reduction:\n",
    "\n",
    "Let's say we have a dataset with 100 samples and 10 features. Each sample \n",
    "represents a person, and the features represent different characteristics\n",
    "such as age, height, weight, income, education level, etc. The goal is to\n",
    "reduce the dimensionality of the dataset while retaining the most important\n",
    "information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe10e10f-1dbe-4774-b4f8-fae7c6015fa4",
   "metadata": {},
   "source": [
    "## Q:4:-What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction ? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7620320f-35ea-4a6c-ac5b-1fc3b893b39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans:-\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique\n",
    "commonly used for feature extraction. It can help uncover the most \n",
    "informative features or patterns in a dataset by transforming the original\n",
    "variables into a new set of uncorrelated variables called principal components.\n",
    "These principal components are ordered in terms of their explanatory power,\n",
    "with the first component capturing the most variance in the data, the second\n",
    "component capturing the second most variance, and so on.\n",
    "\n",
    "PCA can be used for feature extraction by selecting a subset of the principal\n",
    "components that explain a significant portion of the total variance in the data.\n",
    "This process allows us to reduce the dimensionality of the dataset while retaining\n",
    "the most relevant information. By discarding the components with lower variance \n",
    "contributions, we can simplify the representation of the data, remove noise, and\n",
    "enhance interpretability.\n",
    "\n",
    "Here's an example to illustrate how PCA can be used for feature extraction:\n",
    "\n",
    "Let's say we have a dataset containing information about houses, including features \n",
    "such as square footage, number of bedrooms, number of bathrooms, and price. We want \n",
    "to extract the most important features from this dataset.\n",
    "\n",
    "Data Preparation: We start by normalizing the data to ensure that all variables\n",
    "are on the same scale.\n",
    "\n",
    "PCA: We perform PCA on the normalized dataset. The PCA algorithm will calculate\n",
    "the principal components that capture the maximum variance in the data. Each\n",
    "principal component is a linear combination of the original features.\n",
    "\n",
    "Explained Variance: After performing PCA, we examine the explained variance\n",
    "ratio associated with each principal component. The explained variance ratio\n",
    "tells us the proportion of the total variance in the dataset that is captured\n",
    "by each principal component.\n",
    "\n",
    "Selecting Principal Components: We select a subset of the principal components\n",
    "based on the desired amount of variance to retain. For example, if we want to \n",
    "retain 90% of the variance, we choose the first few principal components whose\n",
    "cumulative explained variance exceeds or comes close to this threshold.\n",
    "\n",
    "Feature Extraction: The selected principal components serve as the extracted\n",
    "features. We can discard the original features and use the principal components\n",
    "as a reduced representation of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b3e6de-ff50-48a7-9dae-3198c46d2bdc",
   "metadata": {},
   "source": [
    "## Q:5:- You are working on a project to build a recommendation system for a food delivery service . The dataset contain feature such as price , rating , and delivery time . Explain how you would use Min-Max scailing to preprocessing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791415fe-a375-4bdd-85af-e10847071d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans:-\n",
    "To preprocess the data for building a recommendation system for a food\n",
    "delivery service using Min-Max scaling, follow these steps:\n",
    "\n",
    "Understand the data: Examine the dataset to gain a clear understanding\n",
    "of its structure, including the features (price, rating, delivery time),\n",
    "their ranges, and any other relevant information.\n",
    "\n",
    "Normalize the features: Since the features have different ranges, it is \n",
    "essential to normalize them to ensure they are on a similar scale.\n",
    "Min-Max scaling is a common technique used for this purpose.\n",
    "\n",
    "Apply Min-Max scaling: Min-Max scaling transforms the data into a specific\n",
    "range, typically between 0 and 1. The formula to perform Min-Max scaling\n",
    "on a feature X is:\n",
    "\n",
    "X_scaled = (X - X_min) / (X_max - X_min)\n",
    "\n",
    "In this case, for each feature, subtract the minimum value (X_min) of that\n",
    "feature from each data point and divide the result by the range of that\n",
    "feature (X_max - X_min).\n",
    "\n",
    "Scaling each feature: Apply the Min-Max scaling formula to each feature \n",
    "in your dataset individually. For example, calculate the scaled values\n",
    "for the price, rating, and delivery time features.\n",
    "\n",
    "Store the scaling parameters: While performing Min-Max scaling, it is\n",
    "crucial to store the minimum and maximum values for each feature. These\n",
    "parameters will be needed later to scale new data points or revert the\n",
    "scaled data back to its original form if required.\n",
    "\n",
    "Use the scaled data for analysis: Once you have applied Min-Max scaling\n",
    "to the features, the data will be on a similar scale. You can now use \n",
    "this scaled data to build your recommendation system. You may employ various \n",
    "techniques like collaborative filtering, content-based filtering, or hybrid \n",
    "approaches to generate recommendations based on the scaled features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa2d560-097f-4a38-a479-16c76e157ea6",
   "metadata": {},
   "source": [
    "## Q:6:- You are working on a project to build a model to predict stock prices . The dataset contains many features, such as company financial data and market trends . Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add37a6b-75c3-45a7-929d-3edc3452f71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans:-\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique\n",
    "commonly used in machine learning to reduce the number of features in a\n",
    "dataset while retaining most of the important information. Here's how you\n",
    "can use PCA to reduce the dimensionality of your dataset for predicting\n",
    "stock prices:\n",
    "\n",
    "Dataset Preparation:\n",
    "\n",
    "Collect a dataset containing various features relevant to stock prices,\n",
    "such as company financial data and market trends.\n",
    "Ensure that the dataset is preprocessed and standardized, which involves\n",
    "scaling the features to have zero mean and unit variance. This step is\n",
    "important for PCA, as it is sensitive to the scale of the features.\n",
    "\n",
    "Covariance Matrix:\n",
    "\n",
    "Compute the covariance matrix of the standardized dataset. The covariance\n",
    "matrix provides information about the relationships between different features.\n",
    "\n",
    "Eigendecomposition:\n",
    "\n",
    "Perform eigendecomposition on the covariance matrix. This step calculates the\n",
    "eigenvalues and eigenvectors of the covariance matrix.\n",
    "The eigenvalues represent the amount of variance explained by each eigenvector,\n",
    "and the eigenvectors represent the directions in which the data vary the most.\n",
    "\n",
    "Selection of Principal Components:\n",
    "\n",
    "Sort the eigenvalues in descending order and select the top 'k' eigenvectors \n",
    "that correspond to the largest eigenvalues.\n",
    "Typically, you would aim to select enough principal components to explain a \n",
    "significant portion of the variance in the data. For example, you may choose\n",
    "the top 'k' eigenvectors that explain, say, 90% or 95% of the total variance.\n",
    "\n",
    "Projection onto Principal Components:\n",
    "\n",
    "Transform the original dataset onto the selected 'k' eigenvectors, forming a \n",
    "new reduced-dimensional dataset.\n",
    "This transformation involves multiplying the standardized dataset with the \n",
    "selected eigenvectors.\n",
    "\n",
    "Model Training and Evaluation:\n",
    "\n",
    "Train your stock price prediction model on the reduced-dimensional dataset\n",
    "obtained from the PCA transformation.\n",
    "Assess the performance of your model using appropriate evaluation metrics,\n",
    "such as mean squared error (MSE), mean absolute error (MAE), or other \n",
    "relevant metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8133be3a-4835-4e3b-8a7e-92dea883a102",
   "metadata": {},
   "source": [
    "## Q:7:- For dataset containing the following values: [1,5,10,15,20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0883f759-9b27-4e5b-8edc-9613469788de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans:-\n",
    "To perform Min-Max scaling on a dataset, you need to follow these steps:\n",
    "\n",
    "Find the minimum and maximum values in the dataset.\n",
    "Subtract the minimum value from each data point.\n",
    "Divide the result from step 2 by the difference between the maximum and minimum values.\n",
    "Multiply the result from step 3 by 2.\n",
    "Subtract 1 from the result from step 4 to shift the values to the range of -1 to 1.\n",
    "Let's apply these steps to the dataset [1, 5, 10, 15, 20]:\n",
    "\n",
    "Minimum value: 1\n",
    "Maximum value: 20\n",
    "\n",
    "Subtract the minimum value from each data point:\n",
    "[1 - 1, 5 - 1, 10 - 1, 15 - 1, 20 - 1] = [0, 4, 9, 14, 19]\n",
    "\n",
    "Divide the result by the difference between the maximum and minimum values:\n",
    "[0 / (20 - 1), 4 / (20 - 1), 9 / (20 - 1), 14 / (20 - 1), 19 / (20 - 1)] =\n",
    "[0, 0.2353, 0.5294, 0.8235, 1]\n",
    "\n",
    "Multiply the result by 2:\n",
    "[0 * 2, 0.2353 * 2, 0.5294 * 2, 0.8235 * 2, 1 * 2] = [0, 0.4706, 1.0588, 1.6471, 2]\n",
    "\n",
    "Subtract 1 from the result:\n",
    "[0 - 1, 0.4706 - 1, 1.0588 - 1, 1.6471 - 1, 2 - 1] = [-1, -0.5294, 0.0588, 0.6471, 1]\n",
    "\n",
    "After performing Min-Max scaling, the transformed dataset within the range of\n",
    "-1 to 1 is [-1, -0.5294, 0.0588, 0.6471, 1].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e67fad2-89cb-4e87-8cb8-b31198092d55",
   "metadata": {},
   "source": [
    "## Q:8:- For dataset containing the following features: [height , weight , age , gender, blood pressure], perform feature Extraction using PCA.How many principal components would you choose to retain , and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1fd3b5-7bda-4263-b34f-db7d6766e1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans:-\n",
    "To determine the number of principal components to retain in PCA \n",
    "(Principal Component Analysis), we need to consider the cumulative \n",
    "explained variance ratio. The explained variance ratio represents the \n",
    "amount of variance in the original data that is explained by each\n",
    "principal component.\n",
    "\n",
    "Here's a general approach to deciding the number of principal \n",
    "components to retain:\n",
    "\n",
    "Standardize the data: Before applying PCA, it is important to standardize\n",
    "the features to have zero mean and unit variance. This step ensures that\n",
    "features with larger magnitudes do not dominate the analysis.\n",
    "\n",
    "Compute the covariance matrix: Calculate the covariance matrix of the\n",
    "standardized data.\n",
    "\n",
    "Perform PCA: Apply PCA on the covariance matrix and obtain the eigenvalues \n",
    "and eigenvectors.\n",
    "\n",
    "Calculate the explained variance ratio: Compute the explained variance ratio\n",
    "for each principal component by dividing its eigenvalue by the sum of all\n",
    "eigenvalues.\n",
    "\n",
    "Determine the number of components to retain: Examine the cumulative explained\n",
    "variance ratio, which is the accumulated sum of the explained variance ratios.\n",
    "Typically, a commonly used threshold is to retain enough principal components\n",
    "to explain a significant portion of the variance, such as 90% or 95%.\n",
    "\n",
    "Based on this information, let's assume you have performed PCA on your dataset.\n",
    "The cumulative explained variance ratio is a plot showing the accumulated \n",
    "explained variance ratio as you add more principal components.\n",
    "\n",
    "For example, let's say the cumulative explained variance ratio plot looks\n",
    "like this:\n",
    "\n",
    "yaml\n",
    "Copy code\n",
    "Cumulative Explained Variance Ratio:\n",
    "1 principal component: 0.4\n",
    "2 principal components: 0.6\n",
    "3 principal components: 0.75\n",
    "4 principal components: 0.9\n",
    "5 principal components: 1.0\n",
    "In this case, you would choose to retain 4 principal components because\n",
    "they explain 90% of the variance in the data. Retaining 4 principal \n",
    "components allows you to reduce the dimensionality of the dataset while\n",
    "retaining most of the important information.\n",
    "\n",
    "It's important to note that the specific number of principal components\n",
    "to retain may vary depending on the dataset and the specific requirements\n",
    "of your analysis. You can adjust the threshold based on your needs and\n",
    "the desired balance between dimensionality reduction and information retention.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f1bd0f-91f0-41f3-b247-988efafa8c86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694fab1b-e18b-41d3-ac9f-5d64b5ea0ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e885d4-b64b-4daf-82e4-0b12c33789cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21eac2b-6b03-4de3-bcc5-316ae5383559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aebf85b-dbe0-46d8-800f-dbbfd197eb06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6431e592-2844-4029-9c35-9dfc7527da42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2682d3-42ec-4834-b09d-2fee6d43884a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964688d5-c499-44aa-b067-912e605d0438",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
